---
title: "EDUC 152. Intro to quantitative research in education: Regression analysis"
subtitle: "Statistical inference"
author: 
date: 
urlcolor: blue
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true # toc_float option to float the table of contents to the left of the main document content. floating table of contents will always be visible even when the document is scrolled
      #collapsed: false # collapsed (defaults to TRUE) controls whether the TOC appears with only the top-level (e.g., H2) headers. If collapsed initially, the TOC is automatically expanded inline when necessary
      #smooth_scroll: true # smooth_scroll (defaults to TRUE) controls whether page scrolls are animated when TOC items are navigated to via mouse clicks
    number_sections: true
    fig_caption: true # ? this option doesn't seem to be working for figure inserted below outside of r code chunk    
    highlight: tango # Supported styles include "default", "tango", "pygments", "kate", "monochrome", "espresso", "zenburn", and "haddock" (specify null to prevent syntax    
    theme: default # theme specifies the Bootstrap theme to use for the page. Valid themes include default, cerulean, journal, flatly, readable, spacelab, united, cosmo, lumen, paper, sandstone, simplex, and yeti.
    df_print: tibble #options: default, tibble, paged
bibliography: ../../assets/bib/educ152_bib.bib
csl: ../../assets/bib/apa.csl
---


<!-- Code to enable scroll right for printing of data frames -->
<style>
pre code, pre, code {
  white-space: pre !important;
  overflow-x: auto !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
</style>


```{r, echo=FALSE, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>", highlight = TRUE, warning = FALSE, message = FALSE)
  #comment = "#>" makes it so results from a code chunk start with "#>"; default is "##"
options(scipen=999)
options(tibble.width = Inf, width = 10000) # Code necessary to enable scroll right for printing of data frames
```

# Introduction

Statistical inference

> The theory, methods, and practice of forming judgments about the parameters of a population and the reliability of statistical relationships, typically on the basis of random sampling 

*credit: [Google](https://www.google.com/search?q=define+%22statistical+inference%22&sxsrf=ALeKk00lHj1hQ5jc8o_Xjsc7vOF6XFMPUg%3A1617292004292&ei=5OplYNCzEdOU-gTl2p_ACQ&oq=define+%22statistical+inference%22&gs_lcp=Cgdnd3Mtd2l6EAMyBwgAEEcQsAMyBwgAEEcQsAMyBwgAEEcQsAMyBwgAEEcQsAMyBwgAEEcQsAMyBwgAEEcQsAMyBwgAEEcQsAMyBwgAEEcQsANQAFgAYPbvD2gBcAJ4AIABvgGIAb4BkgEDMC4xmAEAqgEHZ3dzLXdpesgBCMABAQ&sclient=gws-wiz&ved=0ahUKEwjQtaebst3vAhVTip4KHWXtB5gQ4dUDCA0&uact=5)*


## Libraries we will use

```{r}
#install.packages('tidyverse') # if you haven't installed already
#install.packages('labelled') # if you haven't installed already
#install.packages('patchwork') # if you haven't installed already

library(tidyverse) # load tidyverse package
library(labelled) # load labelled package package
library(patchwork)
```
# Introduce IPEDS data

## What/why IPEDS

**What is IPEDS?**

- ADD TEXT

**Which IPEDS variables will we be using to teach statistical inference**

- Variables about annual cost of attendance for full-time graduate programs
  - average tuition (in-state vs. out-of-state)
  - required fees  (in-state vs. out-of-state)
  - books and supplies; off-campus room and board; off-campus "other expenses"
    - these are calculated for undergraduate students, but should be highly correlated with graduate students
  - **Note**: none of these variables include the cost of healthcare
    - Therefore, assuming you must pay for the cost of healthcare, the measures of cost of attendance (COA) we create will understate the total cost of graduate school + life


**Why use IPEDS data rather than *College Scorecard* data? and why these variables?**

- *College Scorecard* brings together variables from many data sources, including IPEDS
  - The most interesting variables in *College Scorecard* are debt and earnings from degree programs, which come from Office of Federal Student Aid and IRS (I think)
- These university-level debt and earnings variables are measured as the mean (or median) of all students who graduated from the university
  - so the underlying data (that we don't see) are student-level; these student-level data are aggregated to the university-level to create measures mean (or median) debt/earnings at that university
  - Using these measures would make teaching statistical inference more confusing (e.g., trying to test hypotheses about the population mean of mean earnings)
- Why teach inferential statistics using IPEDS measures about graduate tuition (and cost of attendance)?
  - These measures are truly measured at the university-level
  - Tuition and cost-of-attendance are the big drivers of student debt, so learning more about how tuition/cost-of-attendance varies across universities will help you make more informed decisions about pursuing graduate education in the future

## Some definitions

Some definitions related to tuition, fees, expenses, etc; from the IPEDS "Student Charges for Full Academic Year" 2019-20 academic year data dictionary [[LINK]](https://nces.ed.gov/ipeds/datacenter/data/IC2019_AY_Dict.zip):

- "Full-time student" (graduate)
  - Graduate - A student enrolled for 9 or more semester credits, or 9 or more quarter credits, or students involved in thesis or dissertation preparation that are considered full time by the institution
- "Academic year"
  - The period of time generally extending from September to June; usually equated to 2 semesters or trimesters, 3 quarters, or the period covered by a 4-1-4 plan
- "Tuition"
  - Amount of money charged to students for instructional services. Tuition may be charged per term, per course, or per credit
- "In-state tuition"
  - The tuition charged by institutions to those students who meet the state's or institution's residency requirements
- "Out-of-state tuition"
  - The tuition charged by institutions to those students who do not meet the state's or institution's residency requirements
- "Required fees"
  - Fixed sum charged to students for items not covered by tuition and required of such a large proportion of all students that the student who does NOT pay the charge is an exception
- "In-state fees"
  - The fees charged by institutions to those students who meet the state's or institution's residency requirements.
- "Out-of-state fees"
  - The fees charged by institutions to those students who do not meet the state's or institution's residency requirements
- "Books and supplies" (undergraduate)
  - Do not include unusual costs for special groups of students (e.g., engineering or art majors), unless they constitute the majority of students at your institution
- "Room charges"
  - The charges for an academic year for rooming accommodations for a typical student sharing a room with one other student. 
- "Board charges"
  - The charge for an academic year for meals, for a specified number of meals per week. 
- "Other expenses"
  - The amount of money (estimated by the financial aid office) needed by a student to cover expenses such as laundry, transportation, entertainment, and furnishings. (For the purpose of this survey room and board and tuition and fees are not included.) 

*Note*: the IPEDS measures of full-time graduate tuition (both in-state and out-of-state) are "average" tuition price across different graduate degree programs (excluding "first-professional" degree programs like law and medicine)

  - e.g., full-time annual tuition price for an MBA program is likely higher than that of an MA in education
  - But in the context of using these measures to teach statistical inference, pretend these prices are not "averages" but rather the official tuition price
  - make the same assumption for variables about fees, books/supplies, living expenses, etc.

*Note*: "required fees" do not include the cost of healthcare (I think)

## Create dataset `df_ipeds_pop`

Load IPEDS dataset

- CONTAINS DATA ON THESE SORTS OF SOURCES
```{r}
load(file = url('https://github.com/anyone-can-cook/educ152/raw/main/data/ipeds/output_data/panel_data.RData'))
```

Create data frame for use in teaching statistical inference
```{r}
df_ipeds_pop <- panel_data %>%
  # keep data from fall 2019
  filter(year == 2019) %>%
  # which universities to keep:
    # 2015 carnegie classification: keep research universities (15,16,17) and master's universities (18,19,20)
  filter(c15basic %in% c(15,16,17,18,19,20)) %>%
  # which variables to keep
  select(instnm,unitid,opeid6,opeid,control,c15basic,stabbr,city,zip,locale,obereg, # basic institutional characteristics
         tuition6,fee6,tuition7,fee7, # avg tuition and fees for full-time grad, in-state and out-of-state
         isprof3,ispfee3,osprof3,ospfee3, # avg tuition and fees for MD, in-state and out-of-state
         isprof9,ispfee9,osprof9,ospfee9, # avg tuition and fees for Law, in-state and out-of-state
         chg4ay3,chg7ay3,chg8ay3) %>% # [undergraduate] books+supplies; off-campus (not with family) room and board; off-campus (not with family) other expenses
  # rename variables; syntax <new_name> = <old_name>
  rename(region = obereg, # revion
         tuit_grad_res = tuition6, fee_grad_res = fee6, tuit_grad_nres = tuition7, fee_grad_nres = fee7, # grad
         tuit_md_res = isprof3, fee_md_res = ispfee3, tuit_md_nres = osprof3, fee_md_nres = ospfee3, # md
         tuit_law_res = isprof9, fee_law_res = ispfee9, tuit_law_nres = osprof9, fee_law_nres = ospfee9, # law
         books_supplies = chg4ay3, roomboard_off = chg7ay3, oth_expense_off = chg8ay3) %>% # [undergraduate] expenses
  # create measures of tuition+fees
  mutate(
    tuitfee_grad_res = tuit_grad_res + fee_grad_res, # graduate, state resident
    tuitfee_grad_nres = tuit_grad_nres + fee_grad_nres, # graduate, non-resident
    tuitfee_md_res = tuit_md_res + fee_md_res, # MD, state resident
    tuitfee_md_nres = tuit_md_nres + fee_md_nres, # MD, non-resident
    tuitfee_law_res = tuit_law_res + fee_law_res, # Law, state resident
    tuitfee_law_nres = tuit_law_nres + fee_law_nres) %>% # Law, non-resident  
  # create measures of cost-of-attendance (COA) as the sum of tuition, fees, book, living expenses
  mutate(
    coa_grad_res = tuit_grad_res + fee_grad_res + books_supplies + roomboard_off + oth_expense_off, # graduate, state resident
    coa_grad_nres = tuit_grad_nres + fee_grad_nres + books_supplies + roomboard_off + oth_expense_off, # graduate, non-resident
    coa_md_res = tuit_md_res + fee_md_res + books_supplies + roomboard_off + oth_expense_off, # MD, state resident
    coa_md_nres = tuit_md_nres + fee_md_nres + books_supplies + roomboard_off + oth_expense_off, # MD, non-resident
    coa_law_res = tuit_law_res + fee_law_res + books_supplies + roomboard_off + oth_expense_off, # Law, state resident
    coa_law_nres = tuit_law_nres + fee_law_nres + books_supplies + roomboard_off + oth_expense_off) # Law, non-resident    

# Add variable labels to the tuit+fees variables and coa variables
  # tuition + fees variables
    var_label(df_ipeds_pop[['tuitfee_grad_res']]) <- 'graduate, full-time, resident; avg tuition + required fees'
    var_label(df_ipeds_pop[['tuitfee_grad_nres']]) <- 'graduate, full-time, non-resident; avg tuition + required fees'
    var_label(df_ipeds_pop[['tuitfee_md_res']]) <- 'MD, full-time, state resident; avg tuition + required fees'
    var_label(df_ipeds_pop[['tuitfee_md_nres']]) <- 'MD, full-time, non-resident; avg tuition + required fees'
    var_label(df_ipeds_pop[['tuitfee_law_res']]) <- 'Law, full-time, state resident; avg tuition + required fees'
    var_label(df_ipeds_pop[['tuitfee_law_nres']]) <- 'Law, full-time, non-resident; avg tuition + required fees'
    
  # COA variables
    var_label(df_ipeds_pop[['coa_grad_res']]) <- 'graduate, full-time, state resident COA; == tuition + fees + (ug) books/supplies + (ug) off-campus room and board + (ug) off-campus other expenses'
    var_label(df_ipeds_pop[['coa_grad_nres']]) <- 'graduate, full-time, non-resident COA; == tuition + fees + (ug) books/supplies + (ug) off-campus room and board + (ug) off-campus other expenses'
    var_label(df_ipeds_pop[['coa_md_res']]) <- 'MD, full-time, state resident COA; == tuition + fees + (ug) books/supplies + (ug) off-campus room and board + (ug) off-campus other expenses'
    var_label(df_ipeds_pop[['coa_md_nres']]) <- 'MD, full-time, non-resident COA; == tuition + fees + (ug) books/supplies + (ug) off-campus room and board + (ug) off-campus other expenses'
    var_label(df_ipeds_pop[['coa_law_res']]) <- 'Law, full-time, state resident COA; == tuition + fees + (ug) books/supplies + (ug) off-campus room and board + (ug) off-campus other expenses'
    var_label(df_ipeds_pop[['coa_law_nres']]) <- 'Law, full-time, non-resident COA; == tuition + fees + (ug) books/supplies + (ug) off-campus room and board + (ug) off-campus other expenses'

df_ipeds_pop %>% glimpse()
```

Show variable labels
```{r}
df_ipeds_pop %>% var_label()
```

Show value labels for variables that are `labelled` class (code note run)
```{r, eval = FALSE}
df_ipeds_pop %>% select(control,locale,region,c15basic) %>% val_labels()
```

## Investigate `df_ipeds_pop`

Investigate data structure

- confirm one observation per unitid
```{r}
df_ipeds_pop %>% group_by(unitid) %>% summarise(n_per_key=n()) %>% ungroup() %>% count(n_per_key)
```

### Tuition/COA at UC campuses

Graduate, state residents
```{r}
df_ipeds_pop %>%
  # keep UC campuses
  filter(unitid %in% c(110398,110635,110644,110653,110662,110671,110680,110699,110705,110714,445188,110699,110398)) %>%
  select(instnm,unitid,tuit_grad_res,fee_grad_res,tuitfee_grad_res,books_supplies,roomboard_off,oth_expense_off,coa_grad_res)
```
Graduate, non-resident
```{r}
df_ipeds_pop %>%
  # keep UC campuses
  filter(unitid %in% c(110398,110635,110644,110653,110662,110671,110680,110699,110705,110714,445188,110699,110398)) %>%
  select(instnm,unitid,tuit_grad_nres,fee_grad_nres,tuitfee_grad_nres,books_supplies,roomboard_off,oth_expense_off,coa_grad_nres)
```

MD, state resident
```{r}
df_ipeds_pop %>%
  # keep UC campuses
  filter(unitid %in% c(110398,110635,110644,110653,110662,110671,110680,110699,110705,110714,445188,110699,110398)) %>%
  select(instnm,unitid,tuit_md_res,fee_md_res,tuitfee_md_res,books_supplies,roomboard_off,oth_expense_off,coa_md_res)
```

MD, non-resident
```{r}
df_ipeds_pop %>%
  # keep UC campuses
  filter(unitid %in% c(110398,110635,110644,110653,110662,110671,110680,110699,110705,110714,445188,110699,110398)) %>%
  select(instnm,unitid,tuit_md_nres,fee_md_nres,tuitfee_md_nres,books_supplies,roomboard_off,oth_expense_off,coa_md_nres)
```


Law, state resident
```{r}
df_ipeds_pop %>%
  # keep UC campuses
  filter(unitid %in% c(110398,110635,110644,110653,110662,110671,110680,110699,110705,110714,445188,110699,110398)) %>%
  select(instnm,unitid,tuit_law_res,fee_law_res,tuitfee_law_res,books_supplies,roomboard_off,oth_expense_off,coa_law_res)
```

Law, non-resident
```{r}
df_ipeds_pop %>%
  # keep UC campuses
  filter(unitid %in% c(110398,110635,110644,110653,110662,110671,110680,110699,110705,110714,445188,110699,110398)) %>%
  select(instnm,unitid,tuit_law_nres,fee_law_nres,tuitfee_law_nres,books_supplies,roomboard_off,oth_expense_off,coa_law_nres)
```

### Tuition/COA at fancy privates

Some fancy-pants private universities

- USC
  - `unitid == 123961`
- Stanford
  - `unitid == 243744`
- Columbia
  - `unitid == 190150`
- Columbia, Teacher's College
  - `unitid == 196468`
- NYU
  - `unitid == 193900`
- Harvard
  - `unitid == 166027`
- Vanderbilt
  - `unitid == 221999`
- University of Pennsylvania
  - `unitid == 215062`
- Northwestern University
  - `unitid == 147767`
- Johns Hopkins University
  - `unitid == 162928`


Graduate students

- Note: no difference in prices between resident and non-resident
- Note: Some universities did not provide `books_supplies`, `roomboard_off`, `oth_expense_off`
- Some ludicrously unrealistic values for `roomboard_off` and `oth_expense_off`
```{r}
# In-state
df_ipeds_pop %>%
  # keep private fancy pants
  filter(unitid %in% c(123961,243744,190150,196468,193900,166027,221999,215062,147767,162928)) %>%
  select(instnm,unitid,tuit_grad_res,fee_grad_res,tuitfee_grad_res,books_supplies,roomboard_off,oth_expense_off,coa_grad_res)

# Out-of-state
df_ipeds_pop %>%
  # keep private fancy pants
  filter(unitid %in% c(123961,243744,190150,196468,193900,166027,221999,215062,147767,162928)) %>%
  select(instnm,unitid,tuit_grad_nres,fee_grad_nres,tuitfee_grad_nres,books_supplies,roomboard_off,oth_expense_off,coa_grad_nres)
```

MD students
```{r}
df_ipeds_pop %>%
  # keep private fancy pants
  filter(unitid %in% c(123961,243744,190150,196468,193900,166027,221999,215062,147767,162928)) %>%
  select(instnm,unitid,tuit_md_res,fee_md_res,tuitfee_md_res,books_supplies,roomboard_off,oth_expense_off,coa_md_res)
```

Law students
```{r}
df_ipeds_pop %>%
  # keep private fancy pants
  filter(unitid %in% c(123961,243744,190150,196468,193900,166027,221999,215062,147767,162928)) %>%
  select(instnm,unitid,tuit_law_res,fee_law_res,tuitfee_law_res,books_supplies,roomboard_off,oth_expense_off,coa_law_res)
```


### Some basic descriptive statistics

Number of institutions by type
```{r}
# Number of institutions
df_ipeds_pop %>% count()

# Number of institutions by Carnegie type
df_ipeds_pop %>% count(c15basic) %>% as_factor()

# Number of institutions by public/private
df_ipeds_pop %>% count(control) %>% as_factor()

# number of institutions by public/private and carnegie type
df_ipeds_pop %>% count(control,c15basic) %>% as_factor()

# number of institutions by level of urbanization
df_ipeds_pop %>% count(locale) %>% as_factor()

# number of institutions by public/private and level of urbanization
df_ipeds_pop %>% count(control,locale) %>% as_factor()
```

Tuition+fees by public/private and Carnegie type
```{r}
df_ipeds_pop %>% group_by(control,c15basic) %>% 
  summarize(
    sample_size = n(),
    n_nonmiss_tuitfee_res = sum(!is.na(tuitfee_grad_res)),
    mean_tuitfee_res = mean(tuitfee_grad_res, na.rm = TRUE),
    n_nonmiss_tuitfee_nres = sum(!is.na(tuitfee_grad_nres)),
    mean_tuitfee_nres = mean(tuitfee_grad_nres, na.rm = TRUE),    
  ) %>% as_factor()
```

Cost of attendance by public/private and Carnegie type
```{r}
df_ipeds_pop %>% group_by(control,c15basic) %>% 
  summarize(
    sample_size = n(),
    n_nonmiss_coa_res = sum(!is.na(coa_grad_res)),
    mean_coa_res = mean(coa_grad_res, na.rm = TRUE),
    n_nonmiss_coa_nres = sum(!is.na(coa_grad_nres)),
    mean_coa_nres = mean(coa_grad_nres, na.rm = TRUE),    
  ) %>% as_factor()
```

# Distributions

**Definitions**

- Probability distribution

> In probability theory and statistics, a probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes

A little easier to get your head around "frequency distribution" and "relative frequency distribution"

- Frequency distribution
  - for each value of a variable, the number of observations that have that value
- Relative frequency distribution
  - for each value of a variable, the percent of observations that have that value (the number of observations with that value divided by the total number of observations)

We can show frequency distributions (or relative frequency distributions) as a table or as a graph

- Frequency distribution of variable `tuit_grad_res` (for first few values)

```{r}
df_ipeds_pop %>% count(tuitfee_grad_res)
```

We can also visualize the distribution of the variable `tuitfee_grad_res`. 

- Instead of "visualize," sometimes we will say "plot" or "draw"


Below, we write a function named `plot_distribution()` that we will call on below to plot variables. Our `plot_distribution()` function calls functions from the `ggplot2` library, which is part of the `tidyverse`

- **Do not** worry about understanding this code!
- I did not write this function. 
- (Props to Crystal Han (data scientist extraordinaire) for writing this function -- and some other functions I use below -- in short order as I was way behind in writing this lecture!)

```{r}
# Function to generate plot
plot_distribution <- function(data_vec, plot_title = '') {
  p <- ggplot(as.data.frame(data_vec), aes(x=data_vec)) + xlab('') +
    ggtitle(plot_title) +
    geom_histogram(aes(y=..density..), alpha=0.4, position='identity') +
    geom_density() +
    geom_vline(aes(xintercept=mean(data_vec, na.rm = T), color='mean'),
               linetype='dotted', size=0.8, alpha=0.8) +
    geom_vline(aes(xintercept=median(data_vec, na.rm = T), color='median'),
               linetype='dotted', size=0.8, alpha=0.8) +
    scale_color_manual(name = 'Statistics',
                       labels = c(paste('Mean:', round(mean(data_vec, na.rm = T), 2),
                                        '\nStd Dev:', round(sd(data_vec, na.rm = T), 2)), 
                                paste('Median:', round(median(data_vec, na.rm = T), 2))),
                       values = c(median='blue', mean='red')) +
    theme(plot.title = element_text(size=10, face='bold', hjust = 0.5),
          legend.title = element_text(size=9, face='bold'),
          legend.text = element_text(size=8))
  
  p
}
```

Call the `plot_distribution()` function to plot the variable `tuitfee_grad_res` from the data frame `df_ipeds_pop`. Essentially, the `plot_distribution()` function creates the following things:

- draws a "histogram" of the variable
- draws a "kernel density estimate," which -- according to the help file for the `geom_density()` function -- is a "smoothed version of the histogram. This is a useful alternative to the histogram for continuous data that comes from an underlying smooth distribution"
- prints the value of the following descriptive statistics: mean, median, standard deviation
- draws dotted vertical lines to indicate the value of the mean and the median

```{r}
plot_distribution(df_ipeds_pop$tuitfee_grad_res)
```




## Normal, right-skew, left-skew


The distributions of variables fall into a few general categories

- we will talk about normally distributed variables, right-skewed variables, and left-skewed variables

<br>

In order to explain these concepts, we will generate a normally distributed variable, a right-skewed variable, and a left-skewed variable 

- We will pretend that each variable contains all observations in the population for some population of interest
  - Usually, we don't know the population; we collect a sample from that population and use the sample data to make statements (inferences) about the unknown population
- we will put each of these variables in a dataset called `def_generated_pop`
- don't worry about below code

```{r}
num_obs <- 10000

# Generate normal distribution w/ custom mean and sd
set.seed(124)
norm_dist <- rnorm(n = num_obs, mean = 50, sd = 5)

# Generate right-skewed distribution
set.seed(124)
rskew_dist <- rbeta(n = num_obs, shape1 = 2, shape2 = 5)

# Generate left-skewed distribution
set.seed(124)
lskew_dist <- rbeta(n = num_obs, shape1 = 5, shape2 = 2)

# Generate standard normal distribution (default is mean = 0 and sd = 1)
set.seed(124)
stdnorm_dist <- rnorm(n = num_obs, mean = 0, sd = 1)  # equivalent to rnorm(10)

# Create dataframe
df_generated_pop <- data.frame(norm_dist, rskew_dist, lskew_dist, stdnorm_dist)

```

We can refer to variables in the data frame `df_generated_pop` using the following syntax:

  - `data_frame_name$variable_name`
  - e.g., `df_generated_pop$norm_dist`

Examine the variable `norm_dist` in the data frame `df_generated_pop`
```{r}
length(df_generated_pop$norm_dist) # length() = number of "elements" = number of observations

#mean
mean(df_generated_pop$norm_dist, na.rm = TRUE)
```


**Normal distribution**

- a normal distribution is "bell shaped"; has symmetric "tails"

We generated a variable `df_generated_pop$norm_dist` that has a normal distribution and then plot the variable to visualize what a normal distribution looks

- Descriptive statistics about the variable `df_generated_pop$norm_dist`
  - It has a mean of `r round(mean(df_generated_pop$norm_dist, na.rm = TRUE), digits = 2)`
  - It has a standard deviation of `r round(sd(df_generated_pop$norm_dist, na.rm = TRUE), digits = 2)`
    - Standard deviation is a measure of how far away from the mean observations tend to be
    - we can interpet this standard deviation as follows: on average, observations are `r round(sd(df_generated_pop$norm_dist, na.rm = TRUE), digits = 2)` away from the mean of `r round(mean(df_generated_pop$norm_dist, na.rm = TRUE), digits = 2)`


We can also visualize the variable `df_generated_pop$norm_dist`, as shown below. Note the following:

- symmetric, "bell" shape
- the mean is (nearly) identical to the median

```{r}
plot_distribution(df_generated_pop$norm_dist)
```

**Skewed distribution**

- for a variable with a "skewed distribution" one "tail" is longer than the other
- "outliers" are often the cause of skewed distributions
  - outliers are observations that are much higher or much lower than most observations
- skewed distributions are either "left skewed" or "right skewed"

**Right skewed distributions**

- The right "tail" is longer than the left due to the presence of positive outliers, defined as observations with very high values compared to most observations
- so there are more positive outliers than you would expect in a bell (normal) shaped variable
- these positive outliers increase the value of the mean, such that the value of the mean is higher than the value of the median
  - mean > median
- real-world variables that tend to be right-skewed
  - income; enrollment size, city population


We generated the right-skewed variable `df_generated_pop$rskew_dist`

- This variable has the following descriptive statistics
  - mean equals `r round(mean(df_generated_pop$rskew_dist, na.rm = TRUE), digits = 3)`
  - median equals `r round(median(df_generated_pop$rskew_dist, na.rm = TRUE), digits = 3)`
  - standard deviation equals `r round(sd(df_generated_pop$rskew_dist, na.rm = TRUE), digits = 3)`
    - interpretation: on average, observations are `r round(sd(df_generated_pop$rskew_dist, na.rm = TRUE), digits = 3)` away from the mean of `r round(mean(df_generated_pop$rskew_dist, na.rm = TRUE), digits = 3)`
- We can also visualize the right-skwed variable `df_generated_pop$rskew_dist`, as shown below. Note the following:
  - The right "tail" is longer than the left tail
  - the mean is larger than the median


```{r}
plot_distribution(df_generated_pop$rskew_dist)
```

  
**Left skewed distributions**

- The left tail is longer than right tail, usually due to the presence of more negative outliers than would be expected in a bell shaped variable
  - where negative outliers are defined as observations with very low values (e.g., extreme negative values) compared to most observations
- these negative outliers decrease the value of the mean, such that the value of the mean is lower than the value of the median
- In social science research left-skewed variables are less common than right-skewed variables

We generated the left-skewed variable `df_generated_pop$rskew_dist`

- This variable has the following descriptive statistics
  - mean equals `r round(mean(df_generated_pop$lskew_dist, na.rm = TRUE), digits = 3)`
  - median equals `r round(median(df_generated_pop$lskew_dist, na.rm = TRUE), digits = 3)`
  - standard deviation equals `r round(sd(df_generated_pop$lskew_dist, na.rm = TRUE), digits = 3)`
    - interpretation: on average, observations are `r round(sd(df_generated_pop$lskew_dist, na.rm = TRUE), digits = 3)` away from the mean of `r round(mean(df_generated_pop$lskew_dist, na.rm = TRUE), digits = 3)`
- We can also visualize the left-skwed variable `df_generated_pop$rskew_dist`, as shown below. Note the following:
  - The left "tail" is longer than the right tail
  - the mean is smaller than the median

Create and plot left-skewed variable
```{r}
plot_distribution(df_generated_pop$lskew_dist)
```


Having plotted generated variables that have normal, right skewed, and left skewed distributions, respectively, let's plot the real-life variable `tuitfee_grad_res` from the data frame `df_ipeds_pop` (below). How would you diagnose the distribution of `tuitfee_grad_res`?

```{r}
plot_distribution(df_ipeds_pop$tuitfee_grad_res)
```
How would you diagnose the distribution of `tuitfee_grad_res`?

- right tail longer than the left
- mean of `r round(mean(df_ipeds_pop$tuitfee_grad_res, na.rm = TRUE), digits = 0)` is larger than the median of `r round(median(df_ipeds_pop$tuitfee_grad_res, na.rm = TRUE), digits = 0)`
- so this is a right skewed variable

## Properties of the normal distribution

The normal distribution -- symmetric, bell-shaped, mean equal to the median - has very useful properties that make it the basis of inferential statistics

- if we can reasonably assume a variable has a normal distribution, then we know a lot about that variable

```{r}
plot_distribution(df_generated_pop$norm_dist)
```

### Normal distribution and "the empirical rule"

Recall our primary measure of dispersion, standard deviation, which measures how far away individual observations tend to be from the mean.

- Here, for some variable $x$, I'll give the formula for sample standard deviation, $\hat\sigma_x$, as opposed to the formula for population standard deviation, $\sigma_x$
  - sometimes people will also refer to sample standard deviation of $x$ as $s_x$


- $\hat\sigma_x = \sqrt{\frac{\sum_{i=1}^n (x_i - \overline{x})^2}{n-1}}$

**The empirical rule** states that if variable has an approximately normal distribution (i.e., approximately “bell shaped”) then:

- About 68% of obs fall within one std. dev of mean  
  - i.e., between $x - \hat{\sigma{x}}$ and $x + \hat{\sigma{x}}$
  
- About 95% of obs fall within two std. dev of mean  
  - i.e., between $x - 2\hat\sigma_x$ and $x + 2\hat\sigma_x$
  
- About 99% of obs fall within three std. dev of mean  
  - i.e., between $x - 3\hat\sigma_x$ and $x + 3\hat\sigma_x$

<br>
<!--Show picture from old "chicken scratch" pdf conveying ideas about empirical rule
<br>
-->

Why is the empirical rule so important for inferential statistics?

- basically, if a variable has an approximately normal distribution, then we know how likely it would be to observe a variable that is a certain number of standard deviations away from the mean
- for example: 
  - only about 2.5% of observations have a value higher than two standard deviations or more from the mean; 
  - the variable `norm_dist` has a mean of about `50` and a standard deviation of about `5`, so the value of `40` would be about two standard deviations below the mean. the empirical rule tells us that only about 2.5% of observations would have a value less than `40`
- you might say, but most real-life variables are unlikely to have a normal distribution
  - True! But the "sampling distribution" -- discussed below -- which is the basis for all inferential statistics/hypothesis testing, **always** has a normal distribution so long as our sample size is large enough
  
  
### Z-scores

The "z-score" of an observation is the number of standard deviations away from the mean

- can define a z-score in terms of a population or a sample from that population; we'll introduce z-score in the context of sample data

z-score formula

- where $x$ is some variable of interest; subscript $i$ refers to observations
- $z_i = (x_i - \bar{x})/(\hat{\sigma}_x)$
- in words:
  - z score for observation $i$ equals the difference between the observation $x_i$ and the mean $\bar{x}$ divided by the standard deviation $\hat{\sigma}_x$
- Intuition behing z-score
  - it's just the difference between an observation value and the mean value, scaled in terms of standard deviations
  - that's why we say that the z-score represents the number of standard deviations away from the mean

Calculating z-score for the variable `norm_dist` from data frame `df_generated_pop`
```{r}
# components of z-score
mean(df_generated_pop$norm_dist, na.rm = TRUE)
sd(df_generated_pop$norm_dist, na.rm = TRUE)

#create new variable z_norm_dist
df_generated_pop <- df_generated_pop %>% mutate(
  z_norm_dist = (norm_dist - mean(norm_dist, na.rm = TRUE))/sd(norm_dist, na.rm = TRUE)
)

#list a few observations
df_generated_pop %>% select(norm_dist,z_norm_dist)

# mean of z-score variable
round(mean(df_generated_pop$z_norm_dist, na.rm = TRUE), digits = 4)
```

Plot the new z-score variable, which has:

- mean of about `0`
- standard deviation of about `1`
```{r}
plot_distribution(df_generated_pop$z_norm_dist)
```

Recall empirical rule for normally distributed variables

- About 68% of obs fall within one std. dev of mean  
  - i.e., between $x - z$ and $x + z$

- About 95% of obs fall within two std. dev of mean  
  - i.e., between $x - 2z$ and $x + 2z$
  
- About 99% of obs fall within three std. dev of mean  
  - i.e., between $x - 3z$ and $x + 3z$


<br>
Show picture from old "chicken scratch" pdf conveying ideas about z-score and empirical rule
<br>

Delete variable `z_norm_dist`
```{r}
df_generated_pop$z_norm_dist <- NULL
```

## Standard normal distribution

Definition:

- a bell-shaped (i.e., normal) distribution that has a mean of `0` and a standard deviation of `1`


Above, we created a variable `stdnorm_dist` in the data frame `df_generated_pop` that has a standard normal distribution. Let's investigate and plot this variable:
```{r}
mean(df_generated_pop$stdnorm_dist, na.rm = TRUE)
sd(df_generated_pop$stdnorm_dist, na.rm = TRUE)

plot_distribution(df_generated_pop$stdnorm_dist)
```

Commentary about standard normal distribution:

- The value of each observation is already in terms of z-scores
- That is, the value of each observation shows how many standard deviations it is from the mean
- Question: if the variable has a standard normal distribution, would it be likely to see an observation with a value of 3?
  - No. because a value of 3 would mean that the observation is three standard deviations greater than the mean. we know that for any variable with a normal distribution, less than 1% of observations have a value that is three standard deviations greater than the mean.
  
  
Question: If we have a variable with a roughly normal distribution (i.e., symmetrical), how could we transform it into a variable with a standard normal distribution (i.e., symmetrical with mean=0 and std deviation=1)?

- Answer: create new variable that is the z-score associated with each value
- This is exactly, what we did above (shown again here)

```{r}
# create variable that has a standard normal distribution from a variable that has a normal distribution
df_generated_pop %>% mutate(
  z_norm_dist = (norm_dist - mean(norm_dist, na.rm = TRUE))/sd(norm_dist, na.rm = TRUE)
)
```

# Sampling distribution

The "sampling distribution" is the fundamental concept of inferential statistics

Briefly, recall the goal of inferential statistics:

- We want to make statements about population parameters, for example the population mean of some variable $x$, $\mu_x$
  - But we usually cannot obtain data on the entire population
- Therefore, we collect a representative (random) sample from the population
- We calculate "estimates" based on this sample data. These estimates are our best guess abut the value of population parameters
- For example, the sample mean $\bar{x}$ is our best guess of the population mean $\mu_x$

Usually, we collect a single sample from the population. How do we know if the sample we collected is representative of the underlying population we want to make statements about?

- This is a problem that statisticians have thought a lot about


For example, for our variable `norm_dist` from the data frame `df_generated_pop`, we randomly draw `30` observations from a population of `10,000` observations
```{r}
set.seed(321)
norm_dist_s1 <- sample(x = df_generated_pop$norm_dist, size = 30)

mean(df_generated_pop$norm_dist)
mean(norm_dist_s1) # mean of our sample
```
But, what if we had obtained a different random sample?
```{r}
set.seed(123)
norm_dist_s1 <- sample(x = df_generated_pop$norm_dist, size = 30)

mean(df_generated_pop$norm_dist)
mean(norm_dist_s1) # mean of our sample
```
So we can see that the sample mean, $\bar{x}$, changes from sample to sample

<br>

Imagine if we take 1,000 random samples of size `n` (e.g., `30`) from a population

- for each random sample, we calculate the sample mean, and record the value of the sample mean
- we would have 1,000 observations, where each observation is the value of a sample mean
- If we plotted these 1,000 observations, it would give us a distribution of sample means
- More specifically, this would give us the "sampling distribution" of the sample mean


<br>

**Sampling distribution (of the sample mean)**

- The sampling distribution of the sample mean is a relative frequency distribution where each observation is the sample mean of a single random sample from a population
- The sampling distribution shows how the value of the sample mean varies from sample to sample


<br>

**Excellent website for understanding how the sampling distribution works**

- This very useful website does interactive simulations that show how the sampling distribution works [LINK](https://onlinestatbook.com/stat_sim/sampling_dist/index.html)
  - **Please** spend 5 minutes playing around with this website; this is really the most important concept in statistics



<br>

The sampling distribution of any statistic

- So far, we have discussed the sampling distribution of the sample mean, but a sampling distribution can be created for any sample statistic (e.g., median, min, max, regression coefficient)
- Once we get to the unit on regression, we'll be thinking about the sampling distribution of a regression coefficient. But the underlying concepts will be exactly the same as the sampling distribution of the sample mean



<br>

## Plot sampling distribution

**Plotting a single random sample and a sampling distribution**

- We are pretending our two data frames `df_generated_pop` and `df_ipeds_pop` contain all observations in the population
- For each of these datasets we will create a version that is a single random sample
- We will plot this single random sample
- Then we will take a large number of random samples, calculate the sample mean for each, and plot these sample means to create the sampling distribution


Create "sample" versions of our two datasets
```{r}
set.seed(124)

# create sample version of our generated data
df_generated_sample <- df_generated_pop[sample(nrow(df_generated_pop), 100), ]
df_generated_sample %>% glimpse()

# create sample version of our ipeds data
df_ipeds_sample <- df_ipeds_pop[sample(nrow(df_ipeds_pop), 100), ]
```

Plot the variable `norm_dist` from the sample dataset
```{r}
plot_distribution(df_generated_sample$norm_dist)
```
Plot the variable `tuitfee_grad_res` from the sample IPEDS dataset
```{r}
plot_distribution(df_ipeds_sample$tuitfee_grad_res)
```


Write function to get the sampling distribution from a variable (defaults equal 500 samples of size 100)
```{r}
# Function to get sampling distribution (default: 500 samples of size 100)
get_sampling_distribution <- function(data_vec, num_samples = 500, sample_size = 100) {
  sample_means <- vector(mode = 'numeric', num_samples)

  for (i in 1:length(sample_means)) {
    samp <- sample(data_vec, sample_size)
    sample_means[[i]] <- mean(samp, na.rm = T)
  }

  sample_means
}
```

Plot the sampling distribution of of the sample mean for the normally distributed variable `norm_dist`
```{r}
plot_distribution(get_sampling_distribution(df_generated_pop$norm_dist))
```

Plot the sampling distribution of of the sample mean for the right-skewed variable `rskew_dist`
```{r}
plot_distribution(get_sampling_distribution(df_generated_pop$rskew_dist))
```
Create visualization that stacks three plots on top of one another: 1= population distribution; 2 = distribution of a single random sample; 3 = sampling distribution

- do this for the normally distributed variable  `norm_dist`
```{r}
plot_distribution(df_generated_pop$norm_dist, plot_title = 'Population distribution') +
  plot_distribution(df_generated_sample$norm_dist, plot_title = 'Single sample distribution') +
  plot_distribution(get_sampling_distribution(df_generated_pop$norm_dist),
                    plot_title = 'Sampling distribution') +
  plot_layout(ncol = 1)
```

Create visualization that stacks three plots on top of one another: 1= population distribution; 2 = distribution of a single random sample; 3 = sampling distribution

- do this for the right-skewed variable  `rskew_dist`
- Notice that even though the underlying population is right skewed and the sample is right-skewed, the sampling distribution has a normal distribution
```{r}
plot_distribution(df_generated_pop$rskew_dist, plot_title = 'Population distribution') +
  plot_distribution(df_generated_sample$rskew_dist, plot_title = 'Single sample distribution') +
  plot_distribution(get_sampling_distribution(df_generated_pop$rskew_dist),
                    plot_title = 'Sampling distribution') +
  plot_layout(ncol = 1)
```

Create visualization that stacks three plots on top of one another: 1= population distribution; 2 = distribution of a single random sample; 3 = sampling distribution

- do this for the right-skewed variable  `tuitfee_grad_res` from IPEDS
- Notice that even though the underlying population is right skewed and the sample is right-skewed, the sampling distribution has a normal distribution
```{r}
plot_distribution(df_ipeds_pop$tuitfee_grad_res, plot_title = 'Population distribution') +
  plot_distribution(df_ipeds_sample$tuitfee_grad_res, plot_title = 'Single sample distribution') +
  plot_distribution(get_sampling_distribution(df_ipeds_pop$tuitfee_grad_res),
                    plot_title = 'Sampling distribution') +
  plot_layout(ncol = 1)
```

## Central limit theorem

**Central limit theorem**


- For random sampling with a large sample size, $n$, the sampling distribution of the sample mean is approximately normally distributed
- In other words, no matter what the distribution of the variable, the sampling distribution will have a normal distribution

What counts as a “large” sample size?

- some say `n=30` or more

Why is central limit theorem important?

- We conducting hypothesis tests about a population parameter (e.g., about a population mean, about a population regression coefficient), based on the sampling distribution of the relevant sample statistic
- even if the underlying variable of interest has a skewed population, the sampling distribution of the sample statistic (e.g., sample mean) will have a normal distribution
- if the sampling distribution has a normal distribution, then we know the percent of observations that are a certain number of standard deviations from the mean

Show central limit theorem using interactive simulation

- [LINK](https://onlinestatbook.com/stat_sim/sampling_dist/index.html)

<br>

Show central limit theorem using a very skewed variable: non-resident, grad school cost of attendance

```{r}

plot_distribution(df_ipeds_pop$tuitfee_grad_nres, plot_title = 'Population distribution') +
  plot_distribution(df_ipeds_sample$tuitfee_grad_nres, plot_title = 'Single sample distribution') +
  plot_distribution(get_sampling_distribution(df_ipeds_pop$tuitfee_grad_nres),
                    plot_title = 'Sampling distribution') +
  plot_layout(ncol = 1)
```

## Standard error

Sample standard deviation, of some variable $Y$

- Sample standard deviation, $\hat{\sigma}_Y$, is the average distance between a random observation and the sample mean $\bar{Y}$
- $\hat{\sigma}_Y = \sqrt{\frac{\sum_{i=1}^n (Y_i - \overline{Y})^2}{n-1}}$$

Sample standard error, of the sample mean, $\bar{Y}$

- Sample standard error, $\hat{\sigma}_{\bar{Y}}$, is based on the sampling distribution of the sample mean $\bar{Y}$
- Sample standard error, $\hat{\sigma}_{\bar{Y}}$, is the average distance between one random sample mean, $\bar{Y}$ and the mean of the sample means, $\bar{Y}_{\bar{Y}}$
  - note: assuming random sampling and a sufficiently large sample size, the mean of sample means, $\bar{Y}_{\bar{Y}}$, is equal to the population mean, $\mu_{Y}$
- In other words, Sample standard error, $\hat{\sigma}_{\bar{Y}}$, is the standard deviation of the sampling distribution
- Mathematically
  - $\hat{\sigma}_{\bar{Y}} = \hat{\sigma}_{Y}/\sqrt{n}$
  - equals sample standard deviation divided by the square root of sample size

<br>

Why is standard error so important?

- Example: percent of people who will vote for Obama in 2012 election (I wrote these bullets a long time ago...)
- Standard error tells us how much statistics derived from a sample are likely to diverge from population parameters
- Sample mean, $\bar{Y}$, is best estimate of the population mean, $\mu_Y$,, the percent of people in the population who will vote for Obama
- Standard error provides an indication of how far away each sample mean is likely to be from the population mean
- Supporse: 
    – Standard error=10%: On average, the sample mean from each poll is likely to be 10% away from population mean
    – Standard error=2%: On average, the sample mean from each poll is likely to be 2% away from population mean

<br>

Do we want standard error to be large or small? Why?

- We want standard error to be small, because small standard error means that our estimates -- based on sample data -- are likely to be closer to the value of the population parameter of interest
- so, small standard error means more "precise" estimates

<br>

How to make standard error somaller?

- $\hat{\sigma}_{\bar{Y}} = \hat{\sigma}_{Y}/\sqrt{n}$
- Standard error decreases as size of your sample increases
- If you have a large sample size, the sample mean from one random sample is likely to be close to population mean
  – And when sample means are close to close to population mean, then standard error is small
- Example: mean income in US, with sample size of 10 versus sample size of 2,000
- Show in interactive demonstration
    - [LINK](https://onlinestatbook.com/stat_sim/sampling_dist/index.html)
  

# Hypothesis testing about a population mean

## What and Why hypothesis testing

<!-- [CUT THIS?] The goal of inferential statistics is to make statements about a population of interest based on data from a representative sample from the population
 -->

Quantitative research in social sciences often proceeds as follows:

- Develop a research question (which guides our research)
- Develop one (or more) testable hypothesis based on that research question
- Obtain data necessary to test the hypothesis
- Test the hypothesis by applying an appropriate statistical test to the data

Some examples of research questions co-authors and I have answered over the years:

- What is the relationship between state appropriations and nonresident enrollment at public universities [@RN3753]?
- What is the effect of nonresident enrollment growth on the number of resident students enrolled at public research universities [@RN4290]?
- What is the effect of participation in the Mexican American Studies on the probability of high school graduation for students in the Tucscon Unified School District [@RN3292]?
- Are high schools with a higher percentage of white students more likely to receive recruiting visits from university admissions officer than high schools with a lower percentage of white students [@RN4450]?


For each of these journal articles, we answered the research question by developing a "testable hypothesis" and testing that hypothesis using some statistical test

<br>

Developing testable hypothesis is central to univariate statistical analysis (one variable), bivariate statistical analysis (two variables), and multivariate statistical analysis (3+ variables, usually a regression model)

Example hypotheses for univariate, bivariate, multivariate statistical analyses

- Univariate statistics (hypothesis tests about a single population mean)
  - Hypothesis: the average annual cost of attendance for graduate school (tuition + fees + living expenses) is $50,000
- Bivariate statistics [hypothesis tests about comparing two population means]
  - Hypothesis: the average annual cost of attendance for graduate school (tuition + fees + living expenses) at private universities is higher than public universities
  - Hypothesis: the average annual cost of attendance for graduate school (tuition + fees + living expenses) at universities in urban areas is higher than universities in suburban areas
- Multivariate statistics (usually a regression model with one dependent variable, one independent variable of interest, and one or more "control" variables)
  - where dependent variable (Y) = cost of attendance; independent variable of interest (X) = private or public university; control variable = level of urbanization
  - Hypothesis: cost of attendance for graduate school is higher at private universities than public universities, even after controlling for level of urbanization

<br>

Why learn how to do hypothesis testing about a single population mean when this class is supposed to be about regression (and hypothesis tests about regression models)?

- You must learn the general principles/concepts about using point-estimates from sample data to test hypotheses about population parameters
- The simplest practical application of these general principles/concepts is testing hypotheses about the value of a single population mean
- The concepts/steps for hypotheses tests about a single population mean are exactly the same as those for testing hypotheses about regression models


## Overview of steps in hypothesis testing

These are the general steps in hypothesis testing:

1. **Hypothesis**
    - formally state your "null" and "alternative" hypothesis
1. **Assumptions**
    - state assumptions that are relied upon by the statistical test you are using to test your hypothesis
1. **Test statistic**
    - Using some appropriate statistical analysis, calculate the "test statistic" necessary to test your hypothesis
1. **p-value (means probability value)**
    - calculate the probability of observing a test statistic as large or larger as the one you calculated
1. **Alpha level/rejection region and conclusion**
    - decide on the "alpha level," the p-value associated with rejection of the null hypothesis
    - compare the p-value you you observed to the alpha level and make a conclusion about your hypothesis test

<br>

In real research projects, do researchers always follow these exact steps? In this exact order?

- Yes, they follow these steps
- But researchers do not necessarily follow steps in this exact order
  - e.g., usually, you would decide on an "alpha level" (rejection region) prior to conducting the statistical analysis
- Often, researchers will not write out each step as formally as we will ask you all to do. 
  - We ask you to write out each step to give you practice. Later in the quarter, you won't have to write out each step

Example we will use to introduce steps in hypothesis testing

- The population mean cost of attendance (COA) for full-time (resident) graduate students, $\mu_Y$, is $25,000

<br>

How we will teach you the steps in hypothesis testing in this lecture

- First, Introduce individual steps in detail, so that you develop a deep, conceptual understanding of each step
  - But when thinking about an individual step in detail, it can be hard to remember its relationship to other steps and to hypothesis testing as a whole
- Second, we will do another pratical example, where we work through all steps more quickly
  - so you can get a better sense of the hypothesis testing process as a whole and the relationships between steps


## Hypotheses

This section presents a more formal introduction to hypotheses, focusing on univariate statistical analyses rather than bivariate or multivariate

Recall that the goal of inferential statistics is to make statements about a population of interest based on data from a representative sample from the population. 

- We make a hypotheses about a population parameter (e.g., population mean of variable $Y$ denotes $\mu_Y$)
- Knowing the true value of the population parameter would require having data on all observations in the population
- Usually, we do not have data on the entire population
- We use sample data to test hypotheses about the population

<br>

Definition

- In statistics, a **hypothesis** is a declarative statement about a population.

<br>

In univariate statistical analyses, we make a hypothesis about one population paramaeter (e.g., population mean $\mu_Y$) from one population of interest (e.g., all "research" universities and "master's" universities, as defined by the Carnegie Classification)


```{r}
df_ipeds_pop %>% glimpse()

mean(df_ipeds_pop$coa_grad_res, na.rm = TRUE)
```

### null and alternative hypothesis

When developing a hypothesis for quantitative research, we always specify a **null hypothesis ($H_0$)** AND an **alternative hypothesis ($H_a$)**

<br>

**Null hypothesis ($H_0$)**

- In univariate statistics, a null hypothesis ($H_0$) is a declarative statement that the population parameter has a specific value
- (in words) $H_0:$ the population mean cost of attendance for for full-time (resident) graduate students, $\mu_Y$, is $25,000
- (using symbols) $H_0: \mu_Y = \mu_{Y0} = \$25,000 $
    - where $\mu_{Y0}$ refers to the parameter value associated with the null hypothesis
    - when testing a hypothesis about a single population mean, we can refer to $\mu_{Y0}$ as the "null population mean"

<br>

**Alternative hypothesis ($H_a$)**

- An alternative hypothesis ($H_a$) is a declarative statement that the population parameter falls in some alternative range of values as compared to the value declared by the null hypothesis
- There are two kinds of alternative hypotheses: two-sided; and one-sided
- for a given null hypothesis ($H_0$), there will always be one two-sided alternative hypothesis and two different one-sided hypotheses

Two-sided alternative hypothesis

- (in words) $H_a:$ the population mean mean cost of attendance for for full-time (resident) graduate students, $\mu_Y$, is not equal to $25,000
- (using symbols) $ H_a: \mu_Y \ne \$25,000 $


One-sided alternative hypothesis (mean is greater than $\$25,000$)

- (in words) $H_a:$ the population mean mean cost of attendance for for full-time (resident) graduate students, $\mu_Y$, is greater than $25,000
- (using symbols) $H_a: \mu_Y \gt \$25,000 $

One-sided alternative hypothesis (mean is less than $\$25,000$)

- (in words) $H_a:$ the population mean mean cost of attendance for for full-time (resident) graduate students, $\mu_Y$, is less than $25,000
- (using symbols) $H_a: \mu_Y \lt \$25,000 $

ISSUE - EQUATIONS NOT FORMATTING CORRECTLY, EVEN WHEN I CHANGE INDENTATION ASSOCIATED WITH BULLETS


<br> 

**Example of null and alternative hypotheses for bivariate statistical analysis**

Research question: 

- Is the population mean annual cost of attendance for graduate school at public universities ($\mu_{Y_{{pub}}}$) different from the population mean annual cost of attendance for graduate school at private universities ($\mu_{Y_{{priv}}}$)?

Null and alternative hypotheses

- null hypothesis ($H_0$)
  - (in words): $H_0:$ the population mean annual cost of attendance for graduate school at public universities ($\mu_{Y_{{pub}}}$) is the same as the population mean annual cost of attendance for graduate school at private universities ($\mu_{Y_{{priv}}}$)
  - (symbols): $H_0: \mu_{Y_{{pub}}} = \mu_{Y_{{priv}}}$
- Two-sided alternative hypothesis
  - (in words): $H_a:$ the population mean annual cost of attendance for graduate school at public universities ($\mu_{Y_{{pub}}}$) is different than the population mean annual cost of attendance for graduate school at private universities ($\mu_{Y_{{priv}}}$)
  - (symbols): $H_a: \mu_{Y_{{pub}}} \ne \mu_{Y_{{priv}}}$
- One-sided alternative hypothesis ($ pub < priv$)
  - (in words): $H_a:$ the population mean annual cost of attendance for graduate school at public universities ($\mu_{Y_{{pub}}}$) is less than than the population mean annual cost of attendance for graduate school at private universities ($\mu_{Y_{{priv}}}$)
  - (symbols): $H_a: \mu_{Y_{{pub}}} \lt \mu_{Y_{{priv}}}$
  - note: this is the same as a one-sided hypothesis where we hypothesize $ priv > pub $
- One-sided alternative hypothesis ($pub > priv $)
  - (in words): $H_a:$ the population mean annual cost of attendance for graduate school at public universities ($\mu_{Y_{{pub}}}$) is greater than than the population mean annual cost of attendance for graduate school at private universities ($\mu_{Y_{{priv}}}$)
  - (symbols): $H_a: \mu_{Y_{{pub}}} \gt \mu_{Y_{{priv}}}$
  - note: this is the same as a one-sided hypothesis where we hypothesize $ priv < pub $


### Two-sided or one-sided alternative hypotheses?

In real research projects, we are not usually testing a hypothesis about a single population mean (univariate analysis). Rather, we are usually comparing population means of two different groups (bivariate analysis) or we are examining the relationship between an independent variable and the dependent variable after controlling for other variables (multivariate regression analysis)

<br> 

Prior to conducting analyses, we usually have an expectation/suspicion about the result

- For most bivariate analyses, we usually suspect that one particular group is has a higher mean value than the other
  - e.g., we suspect that mean cost of attendance at private universities
  - this suggests a one-sided alternative hypothesis $H_a$
- For most multivariate analyses, we usually suspect the direction of the relationship between $X$ and $Y$
  - e.g., we expect that "hours spent studying" ($X$) has a positive relationship with "grade point average" ($Y$) rather than thinking "the relationship between hours spent studying ($X$) and grad point average ($Y$) does not equal zero
  - this suggests a one-sided alternative hypothesis $H_a$
  
<br> 

Should we specify two-sided or one-sided alternative hypothesis, $H_a$?

- For univariate and bivariate statistical analyses, researchers specify a two-sided alternative hypothesis more often than a one-sided alternative alternative hypothesis
  - often, researchers specify a two-sided alternative hypothesis even when they strongly believe one particular group has a larger population mean then the other
- For multivariate regression analyses, researchers **always** specify and test two-sided alternative hypotheses
  - even when they strongly believe the relationship between $X$ and $Y$ is positive; and even when they strongly believe the relationship between $X$ and $Y$ is positive
- Why this preference for two-sided alternative hypotheses in real research projects?
  - two-sided alternative hypotheses are more "conservative" than one-sided alternative hypotheses; 
  - that is, if you specify a two-sided alternative hypothesis, $H_a$, and reject the null hypothesis, $H_0$, then it is necessarily true that we would have rejected then null hypothesis, $H_0$, had we specified a one-sided alternative hypothesis, $H_a$
  
<br> 

Because *EDUC152* is a course about regression rather than univariate/bivariate statistics, we will *always* specify two-sided alternative hypotheses and ignore one-sided alternative hypotheses from here on out

- this will make learning hypothesis testing easier


## Test statistic

DEFINE TEST STATISTIC

WHAT YOU WANT CRYSTAL TO CREATE:

changes to make to the set of functions that plot variable distributions; create sampling distribution, etc.

- specify a null hypothesis value that will be associated with a t-test about a hypothesized population mean value
  - e.g., H_0: population mean of coa_grad_res is $25,000
- create sampling distribution under assumption null hypothesis is correct (as opposed to sampling distribution based on pulling n random samples from the population)
- Plots the three graphs on top of one another:
  - population distribution
  - single sample distribution
  - sampling distribution (assuming H_0 is true)
    - add vertical line associated with the value of the t-test statistic (which is calculated based on the data from the single sample we actually observe)
    - function allows you to add vertical lines associated with alpha-level/rejection region for a two-sided alternative hypothesis 
      - e.g. if alpha-level is .05 and two-sided alternative hypothesis, then there would be a vertical line associated with 2.5 percentile (and label t=-1.96 or whatever it is); and a vertical line associated with 97.5 percentile (and label t= 1.96 or whatever it is)
    


```{r}
plot_distribution(df_ipeds_pop$tuitfee_grad_nres, plot_title = 'Population distribution') +
  plot_distribution(df_ipeds_sample$tuitfee_grad_nres, plot_title = 'Single sample distribution') +
  plot_distribution(get_sampling_distribution(df_ipeds_pop$tuitfee_grad_nres),
                    plot_title = 'Sampling distribution') +
  plot_layout(ncol = 1)
```


## p-value

text

## Rejection region and conclusion

text

## Conceptual understanding [MOST IMPORTANT]

text

## Example [all steps]

text

# Fundamentals of causal inference

text

# Hypothesis testing about two populations

text


# References